<!DOCTYPE html><html class="appearance-auto" lang="zh-CN"><head><meta charset="UTF-8"><title>通过堡垒机/跳板机实现文件在服务器之间的互传</title><meta name="description" content="Choose what you love"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="
Scrapy是一个比较好用的Python爬虫框架，你只需要编写几个组件就可以实现网页数据的爬取。但是当我们要爬取的页面非常多的时候，单个服务器的处理能力就不能满足我们的需求了（无论是处理速度还是网络请求的并发数），这时候分布式爬虫的优势就显现出来。
而Scrapy-Redis则是一个基于Redis的Scrapy分布式组件。它利用Redis对用于爬取的请求(Requests)进行存储和调度(Schedule)，并对爬取产生的项目(items)存储以供后续处理使用。scrapy-redi重写了scrapy一些比较关键的代码，将scrapy变成一个可以在多个主机上同时运行的分布式爬虫。
说白了，就是使用redis来维护一个url队列,然后scrapy爬虫都连接这一个redis获取url,且当爬虫在redis处拿.."><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Lei Gao blog" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Lei Gao's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">通过堡垒机/跳板机实现文件在服务器之间的互传</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">点击返回顶部</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile is-hidden"></div><div class="column is-9"><header class="my-4"><a href="/tags/linux"><i class="tag post-item-tag">linux</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">通过堡垒机/跳板机实现文件在服务器之间的互传</h1><time class="has-text-grey" datetime="2022-04-22T06:59:21.000Z">2022-04-22</time><article class="mt-2 post-content"><p><img src="/images/scrapy.png" alt="cover"></p>
<p>Scrapy是一个比较好用的Python爬虫框架，你只需要编写几个组件就可以实现网页数据的爬取。但是当我们要爬取的页面非常多的时候，单个服务器的处理能力就不能满足我们的需求了（无论是处理速度还是网络请求的并发数），这时候分布式爬虫的优势就显现出来。</p>
<p>而Scrapy-Redis则是一个基于Redis的Scrapy分布式组件。它利用Redis对用于爬取的请求(Requests)进行存储和调度(Schedule)，并对爬取产生的项目(items)存储以供后续处理使用。scrapy-redi重写了scrapy一些比较关键的代码，将scrapy变成一个可以在多个主机上同时运行的分布式爬虫。</p>
<p>说白了，就是使用redis来维护一个url队列,然后scrapy爬虫都连接这一个redis获取url,且当爬虫在redis处拿走了一个url后,redis会将这个url从队列中清除,保证不会被2个爬虫拿到同一个url,即使可能2个爬虫同时请求拿到同一个url,在返回结果的时候redis还会再做一次去重处理,所以这样就能达到分布式效果,我们拿一台主机做redis 队列,然后在其他主机上运行爬虫.且scrapy-redis会一直保持与redis的连接,所以即使当redis 队列中没有了url,爬虫会定时刷新请求,一旦当队列中有新的url后,爬虫就立即开始继续爬</p>
<p>首先分别在主机和从机上安装需要的爬虫库</p>
<pre><code>pip3 install requests scrapy scrapy-redis redis</code></pre>
<p>在主机中安装redis</p>
<pre><code>#安装redis
yum install redis

启动服务
systemctl start redis

查看版本号
redis-cli --version

设置开机启动
systemctl enable redis.service</code></pre>
<p>修改redis配置文件 vim /etc/redis.conf 将保护模式设为no，同时注释掉bind，为了可以远程访问,另外需要注意阿里云安全策略也需要暴露6379端口</p>
<pre><code>#bind 127.0.0.1
protected-mode no</code></pre>
<p>改完配置后，别忘了重启服务才能生效</p>
<pre><code>systemctl restart redis</code></pre>
<p>然后分别新建爬虫项目</p>
<pre><code>scrapy startproject myspider</code></pre>
<p>在项目的spiders目录下新建test.py</p>
<pre><code>#导包
import scrapy
import os
from scrapy_redis.spiders import RedisSpider

#定义抓取类
#class Test(scrapy.Spider):
class Test(RedisSpider):

    #定义爬虫名称，和命令行运行时的名称吻合
    name = &quot;test&quot;

    #定义redis的key
    redis_key = &#39;test:start_urls&#39;

    #定义头部信息
    haders = &#123;
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/73.0.3683.86 Chrome/73.0.3683.86 Safari/537.36&#39;
    &#125;

    def parse(self, response):
        print(response.url)
        pass</code></pre>
<p>然后修改配置文件settings.py，增加下面的配置,其中redis地址就是在主机中配置好的redis地址:</p>
<pre><code>BOT_NAME = &#39;myspider&#39;

SPIDER_MODULES = [&#39;myspider.spiders&#39;]
NEWSPIDER_MODULE = &#39;myspider.spiders&#39;

#设置中文编码
FEED_EXPORT_ENCODING = &#39;utf-8&#39;

# scrapy-redis 主机地址
REDIS_URL = &#39;redis://root@39.106.228.179:6379&#39;
#队列调度
SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;
#不清除缓存
SCHEDULER_PERSIST = True
#通过redis去重
DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;
#不遵循robots
ROBOTSTXT_OBEY = False</code></pre>
<p>最后，可以在两台主机上分别启动scrapy服务</p>
<pre><code>scrapy crawl test</code></pre>
<p>此时，服务已经起来了，只不过redis队列中没有任务，在等待状态</p>
<p>进入主机的redis</p>
<pre><code>redis-cli</code></pre>
<p>将任务队列push进redis</p>
<pre><code>lpush test:start_urls http://baidu.com
lpush test:start_urls http://chouti.com</code></pre>
<p>两台服务器的爬虫服务分别领取了队列中的任务进行抓取，同时利用redis的特性，url不会重复抓取</p>
<p>爬取任务结束之后，可以通过flushdb命令来清除地址指纹，这样就可以再次抓取历史地址了。</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2022/05/23/id-22-05/" title="GoLang的Hugo配合nginx来打造属于自己的纯静态博客系."><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">上一页: GoLang的Hugo配合nginx来打造属于自己的纯静态博客系.</span></a><a class="button is-default" href="/2021/05/03/drawboard/" title="通过堡垒机/跳板机实现文件在服务器之间的互传"><span class="has-text-weight-semibold">下一页: 通过堡垒机/跳板机实现文件在服务器之间的互传</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/GLoveLei"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><a title="rss" target="_blank" rel="noopener nofollow" href="/atom.xml"><i class="iconfont icon-rss"></i></a><!-- 知乎--><!-- 领英--><!-- 脸书--></section><p><span>Copyright ©</span><span> Lei Gao 2024</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>